<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Week 3: Video Processing & Live Video Capture - Digital Doubles</title>
    <link rel="stylesheet" href="css/style.css">
    <style>
        .lecture-notes {
            background-color: white;
            padding: 2rem;
            border-radius: 8px;
            margin-top: 2rem;
        }
        .lecture-notes h3 {
            color: var(--primary-color);
            margin-top: 2rem;
            margin-bottom: 1rem;
            font-size: 1.8rem;
        }
        .lecture-notes h4 {
            color: var(--secondary-color);
            margin-top: 1.5rem;
            margin-bottom: 0.75rem;
            font-size: 1.3rem;
        }
        .lecture-notes h5 {
            color: var(--text-color);
            margin-top: 1rem;
            margin-bottom: 0.5rem;
            font-size: 1.1rem;
            font-weight: 600;
        }
        .lecture-notes p {
            margin-bottom: 1rem;
            line-height: 1.7;
        }
        .lecture-notes ul, .lecture-notes ol {
            margin-left: 2rem;
            margin-bottom: 1rem;
        }
        .lecture-notes li {
            margin-bottom: 0.5rem;
        }
        .lecture-notes code {
            background-color: #f4f4f4;
            padding: 0.2rem 0.4rem;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
            font-size: 0.9rem;
        }
        .lecture-notes pre {
            background-color: #f4f4f4;
            border: 1px solid #ddd;
            border-left: 4px solid var(--primary-color);
            padding: 1rem;
            border-radius: 4px;
            overflow-x: auto;
            margin: 1rem 0;
        }
        .lecture-notes pre code {
            background: none;
            padding: 0;
        }
        .lecture-notes a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
        }
        .lecture-notes a:hover {
            text-decoration: underline;
        }
        .lecture-notes .toc {
            background-color: var(--light-bg);
            padding: 1.5rem;
            border-radius: 6px;
            margin: 2rem 0;
        }
        .lecture-notes .toc ol {
            margin-left: 1.5rem;
        }
        .lecture-notes blockquote {
            border-left: 4px solid var(--primary-color);
            padding-left: 1rem;
            margin: 1rem 0;
            font-style: italic;
            color: #666;
        }
        .section-divider {
            border-top: 2px solid var(--border-color);
            margin: 3rem 0;
        }
        .code-example-card {
            background-color: #f8f9fa;
            border: 1px solid #dee2e6;
            border-radius: 8px;
            padding: 1.5rem;
            margin-bottom: 1rem;
        }
        .code-example-card h4 {
            color: var(--secondary-color);
            margin-top: 0;
            margin-bottom: 0.5rem;
        }
        .code-example-card p {
            margin-bottom: 0.5rem;
            color: #666;
        }
    </style>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>Week 3: Video Processing & Live Video Capture</h1>
            <p class="subtitle">November 6, 2025</p>
        </div>
    </header>

    <nav>
        <ul>
            <li><a href="index.html">Home</a></li>
            <li><a href="syllabus.html">Syllabus</a></li>
            <li><a href="weeks.html" class="active">Weekly Materials</a></li>
        </ul>
    </nav>

    <div class="container">
        <section class="content-section">
            <h2>Overview</h2>
            <div class="info-box" style="background-color: #d4edda; border-left: 4px solid #28a745;">
                <p style="margin: 0;"><strong>‚ú® From Still Images to Motion!</strong> This week we extend everything we learned in Week 2 to the temporal dimension, working with video sequences and live camera feeds.</p>
            </div>
            <p style="margin-top: 1.5rem;">
                This week covers video processing fundamentals, live video capture from webcams, real-time effects, interactive parameter control, and basic motion detection. We'll discuss Hito Steyerl's "In Defense of the Poor Image" and its connections to video compression and quality.
            </p>
        </section>

        <section class="content-section">
            <h2>Topics Covered</h2>
            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 2rem;">
                <div>
                    <h3 style="color: var(--primary-color); font-size: 1.3rem;">Part 1: Video Fundamentals</h3>
                    <ul>
                        <li>Understanding video as sequences of frames</li>
                        <li>Video properties (FPS, resolution, codecs)</li>
                        <li>Reading and writing video files</li>
                        <li>Frame-by-frame processing</li>
                        <li>Video formats and compression</li>
                        <li>Processing pre-recorded videos</li>
                    </ul>
                </div>
                <div>
                    <h3 style="color: var(--secondary-color); font-size: 1.3rem;">Part 2: Live Video & Interaction</h3>
                    <ul>
                        <li>Accessing webcam input</li>
                        <li>Real-time video processing</li>
                        <li>Interactive effects with keyboard controls</li>
                        <li>Trackbar parameter adjustment</li>
                        <li>Recording from webcam</li>
                        <li>Motion detection basics</li>
                        <li>Frame differencing techniques</li>
                        <li>Background subtraction</li>
                    </ul>
                </div>
            </div>
        </section>

        <section class="content-section">
            <h2>üíª Code Examples</h2>
            
            <div class="week-card" style="background-color: #f0f7ff; border-left: 4px solid var(--primary-color);">
                <h3>üì¶ Download All Week 3 Code Examples</h3>
                <p>From basic video playback to advanced motion detection and real-time effects!</p>
                <p style="margin-top: 1rem;">
                    <a href="resources/week3_code_examples.zip" class="btn" download style="font-size: 1.1rem; padding: 1rem 2rem;">
                        ‚¨áÔ∏è Download Code Examples (ZIP)
                    </a>
                </p>
                <p style="margin-top: 1rem; font-size: 0.9rem; color: #666;">
                    Includes 10 comprehensive examples with multiple versions, sample videos, and detailed documentation
                </p>
            </div>
            
            <div class="info-box" style="margin-top: 2rem;">
                <h3>üìÅ What's Included: 10 Comprehensive Examples</h3>
                
                <ol style="margin-left: 1.5rem; line-height: 1.8;">
                    <li><strong>Video Capture Basics</strong> - Reading video files and webcam access fundamentals</li>
                    
                    <li><strong>Video Keyboard Effects</strong> - Real-time filters with keyboard controls, includes both comprehensive and simplified versions</li>
                    
                    <li><strong>Trackbar Controls</strong> - Interactive parameter adjustment using trackbar sliders</li>
                    
                    <li><strong>Frame Differencing</strong> - Basic motion detection using frame comparison technique</li>
                    
                    <li><strong>Motion Detection Complete</strong> - Advanced motion detection with bounding boxes, templates, and creative particle effects</li>
                    
                    <li><strong>Background Subtraction</strong> - MOG2 background subtraction for foreground detection</li>
                    
                    <li><strong>Mirror and Flip Effects</strong> - Symmetrical transformations including basic flips and kaleidoscope patterns</li>
                    
                    <li><strong>Color Tracking</strong> - Track objects by color using HSV color space, includes simple demos, mouse-based color selection, object detection with bounding boxes, and motion path tracking</li>
                    
                    <li><strong>Video Recording</strong> - Record from webcam with toggle start/stop controls</li>
                    
                    <li><strong>FPS Counter</strong> - Performance monitoring with reusable display function and flexible positioning</li>
                </ol>

                <p style="margin-top: 1.5rem;"><strong>Also included:</strong></p>

                <ul style="margin-left: 1.5rem;">
                    <li><strong>Resources/</strong> - Sample videos (ocean waves, walking, soccer, ice hockey)</li>
                    <li><strong>README.md</strong> - Complete guide with descriptions of all files and learning tips</li>
                </ul>
            </div>
        </section>

        <section class="content-section">
            <h2>üìö Full Lecture Notes</h2>
            <p style="background-color: #f0f7ff; padding: 1rem; border-radius: 6px; border-left: 4px solid var(--primary-color);">
                <strong>Complete comprehensive lecture notes</strong> covering all Week 3 topics are available below. The notes include detailed explanations, code examples, and conceptual connections to the readings.
            </p>
            
            <div class="lecture-notes">
                <div class="toc">
                    <h4>Table of Contents</h4>
                    <ol>
                        <li><a href="#week-2-recap">Week 2 Recap</a></li>
                        <li><a href="#reading-discussion">Reading Discussion: In Defense of the Poor Image</a></li>
                        <li><a href="#understanding-video">Understanding Digital Video</a></li>
                        <li><a href="#frame-processing">Frame-by-Frame Video Processing</a></li>
                        <li><a href="#live-capture">Live Video Capture</a></li>
                        <li><a href="#realtime-effects">Real-time Processing & Interactive Effects</a></li>
                        <li><a href="#motion-detection">Motion Detection Basics</a></li>
                        <li><a href="#color-tracking">Color Tracking</a></li>
                        <li><a href="#mirror-effects">Mirror Effects & Creative Transformations</a></li>
                        <li><a href="#performance">Performance Monitoring</a></li>
                        <li><a href="#assignments">Week 3 Assignments</a></li>
                        <li><a href="#resources">Additional Resources</a></li>
                    </ol>
                </div>

                <div class="section-divider"></div>

                <h3 id="week-2-recap">Week 2 Recap</h3>
                
                <p>Last week we covered comprehensive image processing - both basic and advanced techniques. Let's quickly review what we learned:</p>

                <h4>Part 1: Basic Image Processing</h4>
                <ul>
                    <li>‚úÖ Loading, displaying, and saving images</li>
                    <li>‚úÖ Color space conversions (BGR, RGB, Grayscale, HSV)</li>
                    <li>‚úÖ Image transformations (resize, rotate, flip, crop)</li>
                    <li>‚úÖ Basic filters and blurring techniques</li>
                    <li>‚úÖ Brightness and contrast adjustment</li>
                    <li>‚úÖ Thresholding (binary, Otsu's, adaptive)</li>
                    <li>‚úÖ Pixel-level manipulation</li>
                    <li>‚úÖ Image compositing</li>
                </ul>

                <h4>Part 2: Advanced Image Processing</h4>
                <ul>
                    <li>‚úÖ Understanding convolution and kernels</li>
                    <li>‚úÖ Custom filter creation</li>
                    <li>‚úÖ Advanced blur techniques</li>
                    <li>‚úÖ Sharpening filters</li>
                    <li>‚úÖ Edge detection (Sobel, Laplacian, Canny)</li>
                </ul>

                <p><strong>Key Takeaway:</strong> Images are just arrays of numbers. By manipulating these numbers mathematically (through operations like convolution), we can transform, enhance, and analyze visual information.</p>

                <h4>This Week: From Still Images to Motion</h4>
                <p>Today we extend everything we learned to the temporal dimension:</p>
                <ul>
                    <li><strong>Part 1:</strong> Video fundamentals (sequences of images, frame-by-frame processing)</li>
                    <li><strong>Part 2:</strong> Live video processing (webcam input, real-time effects, interaction)</li>
                </ul>

                <div class="section-divider"></div>

                <h3 id="reading-discussion">Reading Discussion: In Defense of the Poor Image</h3>
                
                <p>Before diving into video processing, let's discuss last week's reading by Hito Steyerl.</p>

                <h4>Discussion Questions</h4>
                <ol>
                    <li>How does Steyerl's argument challenge the values we've been learning?</li>
                    <li>What's the relationship between image quality and truth?</li>
                    <li>Who benefits from "poor" vs. "rich" images?</li>
                    <li>How might this affect video processing?</li>
                </ol>

                <div class="section-divider"></div>

                <h3 id="understanding-video">Understanding Digital Video</h3>

                <h4>Video = Sequence of Images</h4>
                <p>At its core, <strong>digital video</strong> is simply a sequence of still images (frames) played rapidly enough to create the illusion of continuous motion.</p>

                <p><strong>Key Concepts:</strong></p>
                <ul>
                    <li><strong>Frame:</strong> A single image in a video sequence</li>
                    <li><strong>Frame Rate (FPS):</strong> Frames displayed per second
                        <ul>
                            <li>24 fps - Film standard, "cinematic" look</li>
                            <li>30 fps - TV/video standard</li>
                            <li>60 fps - Smooth motion, gaming, sports</li>
                        </ul>
                    </li>
                    <li><strong>Resolution:</strong> Dimensions of each frame (e.g., 1920√ó1080)</li>
                    <li><strong>Bit Rate:</strong> Amount of data per second</li>
                </ul>

                <h4>Video as a 3D Array</h4>
                <p>Images are 2D arrays (height √ó width) or 3D arrays (height √ó width √ó channels). <strong>Video adds time:</strong></p>
                <pre><code>Video = (frames √ó height √ó width √ó channels)

Example: 10 seconds of HD color video at 30 fps
frames = 10 * 30 = 300 frames
Total array: (300, 1080, 1920, 3)
Total pixels: 1,866,240,000 values!</code></pre>

                <p>This is why video compression is essential and why "poor images" are partly a technical necessity.</p>

                <h4>Video Formats and Codecs</h4>
                <p><strong>Container Formats</strong> (file extension): .mp4, .avi, .mov, .mkv, .webm</p>
                <p><strong>Codecs</strong> (compression algorithms): H.264/AVC, H.265/HEVC, VP9, MJPEG</p>

                <p>Think of it this way:</p>
                <ul>
                    <li><strong>Container</strong> = box that holds video, audio, metadata</li>
                    <li><strong>Codec</strong> = how the video is compressed inside that box</li>
                </ul>

                <div class="section-divider"></div>

                <h3 id="frame-processing">Frame-by-Frame Video Processing</h3>

                <h4>Reading Video Files</h4>
                <p>OpenCV's <code>VideoCapture</code> class handles video input:</p>

                <pre><code>import cv2

# Open video file
cap = cv2.VideoCapture('video.mp4')

# Check if opened successfully
if not cap.isOpened():
    print("Error: Could not open video")
    exit()

# Get video properties
fps = cap.get(cv2.CAP_PROP_FPS)
width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

print(f"Video: {width}x{height} @ {fps} fps")
print(f"Duration: {frame_count/fps:.2f} seconds")

cap.release()</code></pre>

                <p><a href="https://docs.opencv.org/master/d8/dfe/classcv_1_1VideoCapture.html" target="_blank">cv2.VideoCapture() Documentation</a></p>

                <h4>Reading Frames</h4>
                <pre><code>while True:
    ret, frame = cap.read()
    
    if not ret:
        break
    
    # Process frame (it's just a regular image!)
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    
    cv2.imshow('Video', frame)
    
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()</code></pre>

                <p><strong>Important:</strong> <code>cap.read()</code> returns two values:</p>
                <ul>
                    <li><code>ret</code> (boolean) - True if frame read successfully</li>
                    <li><code>frame</code> (NumPy array) - The actual frame image</li>
                </ul>

                <h4>Writing Video Files</h4>
                <p>Save processed video using <code>VideoWriter</code>:</p>

                <pre><code>import cv2

cap = cv2.VideoCapture('input.mp4')

# Get properties
fps = int(cap.get(cv2.CAP_PROP_FPS))
width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

# Create output
fourcc = cv2.VideoWriter_fourcc(*'mp4v')
out = cv2.VideoWriter('output.mp4', fourcc, fps, (width, height))

while True:
    ret, frame = cap.read()
    if not ret:
        break
    
    # Process frame
    processed = cv2.GaussianBlur(frame, (15, 15), 0)
    
    # Write frame
    out.write(processed)

cap.release()
out.release()</code></pre>

                <p><a href="https://docs.opencv.org/master/dd/d9e/classcv_1_1VideoWriter.html" target="_blank">cv2.VideoWriter() Documentation</a></p>

                <div class="section-divider"></div>

                <h3 id="live-capture">Live Video Capture</h3>

                <h4>Accessing Your Webcam</h4>
                <p>Use <code>VideoCapture(0)</code> to access your webcam:</p>

                <pre><code>import cv2

# Open default camera (usually 0)
cap = cv2.VideoCapture(0)

if not cap.isOpened():
    print("Error: Could not access camera")
    exit()

print("Press 'q' to quit")

while True:
    ret, frame = cap.read()
    
    if not ret:
        break
    
    cv2.imshow('Live Camera', frame)
    
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()</code></pre>

                <p><strong>Camera Index:</strong></p>
                <ul>
                    <li><code>0</code> - Default camera (usually built-in webcam)</li>
                    <li><code>1</code> - Secondary camera (if available)</li>
                    <li><code>2</code>, <code>3</code>, etc. - Additional cameras</li>
                </ul>

                <h4>Camera Configuration</h4>
                <pre><code># Set resolution
cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)

# Set frame rate
cap.set(cv2.CAP_PROP_FPS, 30)

# Note: Not all cameras support all settings</code></pre>

                <h4>Flipping the Camera Feed</h4>
                <p>Webcam feeds often appear mirrored. Flip horizontally for a "mirror" view:</p>

                <pre><code>frame = cv2.flip(frame, 1)  # Horizontal flip</code></pre>

                <p><strong>Flip Codes:</strong></p>
                <ul>
                    <li><code>1</code> - Horizontal flip (mirror)</li>
                    <li><code>0</code> - Vertical flip</li>
                    <li><code>-1</code> - Both directions</li>
                </ul>

                <div class="section-divider"></div>

                <h3 id="realtime-effects">Real-time Processing & Interactive Effects</h3>

                <h4>Basic Real-time Filter</h4>
                <p>Apply image processing to live video with keyboard controls:</p>

                <pre><code>import cv2

cap = cv2.VideoCapture(0)
mode = 1  # Start with original

print("Controls:")
print("  1 - Original")
print("  2 - Grayscale")
print("  3 - Blur")
print("  4 - Edge Detection")
print("  q - Quit")

while True:
    ret, frame = cap.read()
    if not ret:
        break
    
    frame = cv2.flip(frame, 1)
    
    # Apply effect based on mode
    if mode == 1:
        processed = frame.copy()
    elif mode == 2:
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        processed = cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR)
    elif mode == 3:
        processed = cv2.GaussianBlur(frame, (25, 25), 0)
    elif mode == 4:
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        edges = cv2.Canny(gray, 50, 150)
        processed = cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR)
    
    cv2.imshow('Live Effects', processed)
    
    key = cv2.waitKey(1) & 0xFF
    if key == ord('q'):
        break
    elif key == ord('1'):
        mode = 1
    elif key == ord('2'):
        mode = 2
    # ... etc

cap.release()
cv2.destroyAllWindows()</code></pre>

                <h4>Interactive Parameter Control with Trackbars</h4>
                <p>Use trackbars to adjust parameters in real-time:</p>

                <pre><code>import cv2

cap = cv2.VideoCapture(0)
window_name = 'Interactive Blur'
cv2.namedWindow(window_name)

# Create trackbar for blur amount
cv2.createTrackbar('Blur', window_name, 1, 25, lambda x: None)

while True:
    ret, frame = cap.read()
    if not ret:
        break
    
    frame = cv2.flip(frame, 1)
    
    # Get blur amount from trackbar
    blur_amount = cv2.getTrackbarPos('Blur', window_name)
    blur_amount = max(1, blur_amount)
    if blur_amount % 2 == 0:
        blur_amount += 1
    
    # Apply blur
    if blur_amount > 1:
        blurred = cv2.GaussianBlur(frame, (blur_amount, blur_amount), 0)
    else:
        blurred = frame.copy()
    
    cv2.imshow(window_name, blurred)
    
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()</code></pre>

                <p><strong>Key Functions:</strong></p>
                <ul>
                    <li><code>cv2.createTrackbar()</code> - Create a slider control</li>
                    <li><code>cv2.getTrackbarPos()</code> - Get current slider value</li>
                </ul>

                <h4>Recording Live Video</h4>
                <pre><code>import cv2
import datetime

cap = cv2.VideoCapture(0)
width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
output_path = f"recording_{timestamp}.mp4"

fourcc = cv2.VideoWriter_fourcc(*'mp4v')
out = cv2.VideoWriter(output_path, fourcc, 30, (width, height))

recording = False

print("Press 'r' to start/stop recording, 'q' to quit")

while True:
    ret, frame = cap.read()
    if not ret:
        break
    
    frame = cv2.flip(frame, 1)
    
    if recording:
        out.write(frame)
        # Add recording indicator
        cv2.circle(frame, (30, 30), 15, (0, 0, 255), -1)
        cv2.putText(frame, "REC", (60, 40),
                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
    
    cv2.imshow('Camera Recording', frame)
    
    key = cv2.waitKey(1) & 0xFF
    if key == ord('q'):
        break
    elif key == ord('r'):
        recording = not recording

cap.release()
out.release()
cv2.destroyAllWindows()
print(f"Video saved to: {output_path}")</code></pre>

                <div class="section-divider"></div>

                <h3 id="motion-detection">Motion Detection Basics</h3>

                <h4>Understanding Motion Detection</h4>
                <p><strong>Basic Idea:</strong> Motion occurs when pixels change between frames.</p>

                <p><strong>Simple Method: Frame Differencing</strong></p>
                <ol>
                    <li>Take current frame</li>
                    <li>Take previous frame</li>
                    <li>Compute absolute difference</li>
                    <li>Threshold the difference</li>
                    <li>Find motion regions</li>
                </ol>

                <h4>Simple Frame Differencing</h4>
                <pre><code>import cv2
import numpy as np

cap = cv2.VideoCapture(0)

# Read first frame
ret, prev_frame = cap.read()
prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)
prev_gray = cv2.GaussianBlur(prev_gray, (21, 21), 0)

while True:
    ret, frame = cap.read()
    if not ret:
        break
    
    # Prepare current frame
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    gray = cv2.GaussianBlur(gray, (21, 21), 0)
    
    # Compute absolute difference
    frame_diff = cv2.absdiff(prev_gray, gray)
    
    # Threshold to get motion mask
    _, motion_mask = cv2.threshold(frame_diff, 25, 255, cv2.THRESH_BINARY)
    
    # Clean up with morphological operations
    kernel = np.ones((5, 5), np.uint8)
    motion_mask = cv2.dilate(motion_mask, kernel, iterations=2)
    
    # Find contours
    contours, _ = cv2.findContours(motion_mask, cv2.RETR_EXTERNAL, 
                                  cv2.CHAIN_APPROX_SIMPLE)
    
    # Draw bounding boxes
    for contour in contours:
        if cv2.contourArea(contour) < 500:
            continue
        
        x, y, w, h = cv2.boundingRect(contour)
        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)
    
    cv2.imshow('Motion Detection', frame)
    cv2.imshow('Motion Mask', motion_mask)
    
    prev_gray = gray.copy()
    
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()</code></pre>

                <p><strong>Key Functions:</strong></p>
                <ul>
                    <li><code>cv2.absdiff()</code> - Compute absolute difference</li>
                    <li><code>cv2.findContours()</code> - Find contours in binary image</li>
                    <li><code>cv2.boundingRect()</code> - Get bounding box of contour</li>
                </ul>

                <h4>Background Subtraction</h4>
                <p>OpenCV provides sophisticated background subtraction algorithms:</p>

                <pre><code># Create background subtractor
bg_subtractor = cv2.createBackgroundSubtractorMOG2(detectShadows=True)

while True:
    ret, frame = cap.read()
    if not ret:
        break
    
    # Apply background subtraction
    fg_mask = bg_subtractor.apply(frame)
    
    cv2.imshow('Foreground Mask', fg_mask)
    cv2.imshow('Original', frame)
    
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break</code></pre>

                <p><a href="https://docs.opencv.org/master/d7/d7b/classcv_1_1BackgroundSubtractorMOG2.html" target="_blank">Background Subtraction Documentation</a></p>

                <div class="section-divider"></div>

                <h3 id="color-tracking">Color Tracking</h3>

                <p>Color tracking is a powerful technique for detecting and following specific colors in video. This is especially useful for interactive installations and creative applications.</p>

                <h4>Understanding Color Tracking</h4>
                <p><strong>Basic Concept:</strong></p>
                <ol>
                    <li>Convert image from BGR to HSV color space</li>
                    <li>Define a range of colors to track</li>
                    <li>Create a mask highlighting those colors</li>
                    <li>Find and track objects in that color range</li>
                </ol>

                <p><strong>Why HSV?</strong> HSV (Hue, Saturation, Value) separates color information from brightness, making it more robust for color tracking than RGB/BGR.</p>

                <h4>Beginner-Friendly Color Tracking</h4>
                <p>The code examples folder includes <strong><code>simple_color_tracker.py</code></strong> - a minimal demonstration with just ~50 lines of code:</p>

                <pre><code>import cv2
import numpy as np

# Define color range in HSV (defaults to blue)
lower_color = np.array([100, 50, 50])
upper_color = np.array([130, 255, 255])

cap = cv2.VideoCapture(0)

while True:
    ret, frame = cap.read()
    if not ret:
        break
    
    # Convert to HSV
    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
    
    # Create mask
    mask = cv2.inRange(hsv, lower_color, upper_color)
    
    # Apply mask
    result = cv2.bitwise_and(frame, frame, mask=mask)
    
    cv2.imshow('Tracked Color', result)
    
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()</code></pre>

                <h4>Interactive Color Selection</h4>
                <p><strong>NEW!</strong> The code examples include <strong><code>mouse_input_color_tracker.py</code></strong> which lets you click on the webcam feed to select any color to track. Features:</p>
                <ul>
                    <li>Click anywhere to select a color</li>
                    <li>Automatic tolerance ranges</li>
                    <li>Shows selected color in indicator square</li>
                    <li>Three views: Original, Mask, Result</li>
                </ul>

                <h4>Object Detection with Color</h4>
                <p><strong>NEW!</strong> The <strong><code>color_object_tracker.py</code></strong> example:</p>
                <ul>
                    <li>Detects and tracks multiple colored objects</li>
                    <li>Draws green bounding boxes around objects</li>
                    <li>Shows blue centroids (center points)</li>
                    <li>Displays object count and area</li>
                </ul>

                <h4>Motion Path Tracking</h4>
                <p><strong>NEW!</strong> The <strong><code>color_path_tracker.py</code></strong> example:</p>
                <ul>
                    <li>Tracks motion path of colored objects</li>
                    <li>Draws blue motion trail with varying thickness</li>
                    <li>Stores last 100 positions</li>
                    <li>Press 'r' to reset path</li>
                </ul>

                <h4>HSV Color Space</h4>
                <p>Understanding HSV helps with color tracking:</p>
                <ul>
                    <li><strong>Hue (H):</strong> The color itself (0-179 in OpenCV)
                        <ul>
                            <li>Red: 0-10, 170-179</li>
                            <li>Green: 40-80</li>
                            <li>Blue: 100-130</li>
                        </ul>
                    </li>
                    <li><strong>Saturation (S):</strong> Color intensity (0-255)</li>
                    <li><strong>Value (V):</strong> Brightness (0-255)</li>
                </ul>

                <div class="section-divider"></div>

                <h3 id="mirror-effects">Mirror Effects & Creative Transformations</h3>

                <p>Mirror and flip effects create symmetrical patterns and can be combined for kaleidoscope-like visuals.</p>

                <h4>Basic Flipping</h4>
                <p><strong>NEW!</strong> The code examples include <strong><code>flip_webcam.py</code></strong> - a simplified version focusing on basic flips:</p>

                <pre><code>import cv2

cap = cv2.VideoCapture(0)
mode = 1  # 1=Original, 2=Horizontal, 3=Vertical, 4=Both

while True:
    ret, frame = cap.read()
    if not ret:
        break
    
    if mode == 1:
        display = frame.copy()
    elif mode == 2:
        display = cv2.flip(frame, 1)  # Horizontal
    elif mode == 3:
        display = cv2.flip(frame, 0)  # Vertical
    elif mode == 4:
        display = cv2.flip(frame, -1)  # Both
    
    cv2.imshow('Flip Effect', display)
    
    key = cv2.waitKey(1) & 0xFF
    if key == ord('q'):
        break
    elif key in [ord('1'), ord('2'), ord('3'), ord('4')]:
        mode = int(chr(key))</code></pre>

                <h4>Kaleidoscope Effect</h4>
                <p><strong>NEW!</strong> The code examples include <strong><code>kaleidoscope.py</code></strong> - a dedicated program that creates 4-way symmetrical patterns. The kaleidoscope effect works by:</p>
                <ol>
                    <li>Taking a quarter of the frame</li>
                    <li>Flipping it to create symmetrical patterns</li>
                    <li>Combining the flipped sections into a complete frame</li>
                </ol>
                <p>This creates mesmerizing geometric patterns perfect for understanding image transformations.</p>

                <div class="section-divider"></div>

                <h3 id="performance">Performance Monitoring</h3>

                <p>When working with real-time video, it's important to monitor performance.</p>

                <h4>FPS Counter</h4>
                <p><strong>Frames Per Second (FPS)</strong> tells you how fast your video processing is running.</p>

                <p>The code examples folder includes <strong><code>fps_counter.py</code></strong> with:</p>
                <ul>
                    <li>Reusable <code>display_fps()</code> function</li>
                    <li>Flexible positioning (top_right, top_left, bottom_right, bottom_left)</li>
                    <li>Real-time FPS calculation</li>
                </ul>

                <p><strong>Why FPS Matters:</strong></p>
                <ul>
                    <li>Lower FPS = lag, choppy video</li>
                    <li>Target: 30 FPS for smooth real-time processing</li>
                    <li>Below 15 FPS = noticeable lag</li>
                    <li>Complex effects will reduce FPS</li>
                </ul>

                <div class="section-divider"></div>

                <h3 id="assignments">Week 3 Assignments</h3>

                <h4>Reading Assignment: "The Zoom Gaze"</h4>
                <p><strong>Article:</strong> <a href="https://reallifemag.com/the-zoom-gaze/" target="_blank">"The Zoom Gaze"</a> by Autumn Caines - Real Life Magazine</p>

                <p><strong>Reading Response (500 words max):</strong></p>
                <p>Autumn Caines examines how video conferencing platforms like Zoom change our relationship with our own image. Please read the essay and write a thoughtful response addressing:</p>

                <ol>
                    <li>What is the "Zoom gaze"? How does Caines describe seeing yourself on camera?</li>
                    <li>How does this relate to live video processing?</li>
                    <li>What power dynamics exist in video platforms? Who controls how we're seen?</li>
                    <li>How do video filters connect to Caines' arguments?</li>
                    <li>How has learning video processing changed your thinking about video calls?</li>
                </ol>

                <p><strong>Due:</strong> November 14, 2025 (next class)</p>
                <p><strong>Submission:</strong> <a href="https://drive.google.com/drive/u/0/folders/1QBC_GxFM-6KoTL7bqZjC76_r6MVasSG8" target="_blank">Week 3 Reading Responses Google Drive folder</a></p>

                <div class="section-divider"></div>

                <h4>Lab Assignment 2: Video Processing Pipeline</h4>
                <p>Create a Python program that processes video (recorded or live) with multiple effects and interaction.</p>

                <h5>Requirements</h5>
                <p>Your program must include:</p>

                <ol>
                    <li><strong>Video Input</strong> - Process either a video file OR live camera feed</li>
                    <li><strong>At least THREE different video processing effects</strong>, such as:
                        <ul>
                            <li>Real-time filters (blur, edge detection, color effects)</li>
                            <li>Motion detection visualization</li>
                            <li>Frame-by-frame artistic effects</li>
                            <li>Interactive parameter control</li>
                        </ul>
                    </li>
                    <li><strong>User Interaction</strong> - Keyboard controls for switching effects, display current mode</li>
                    <li><strong>Performance</strong> - Code should run smoothly with no major lag</li>
                    <li><strong>Code Quality</strong> - Well-organized functions, clear comments, proper cleanup</li>
                </ol>

                <h5>Deliverables</h5>
                <p>Create a folder "YourName_Lab2" in the <a href="https://drive.google.com/drive/u/0/folders/1QBC_GxFM-6KoTL7bqZjC76_r6MVasSG8" target="_blank">Week 3 Assignments Google Drive folder</a> containing:</p>

                <ol>
                    <li><strong>Python script</strong> (.py file) with your video processing code</li>
                    <li><strong>Sample video file</strong> if using pre-recorded video (or link if too large)</li>
                    <li><strong>Output video or screenshots</strong> showing different effects</li>
                    <li><strong>Brief written reflection</strong> (250-300 words) addressing:
                        <ul>
                            <li>What was different about processing video vs. images?</li>
                            <li>What performance challenges did you encounter?</li>
                            <li>What creative possibilities does real-time processing open up?</li>
                            <li>How did you use AI assistance (if at all)?</li>
                        </ul>
                    </li>
                </ol>

                <p><strong>Due:</strong> November 14, 2025 (next class)</p>

                <div style="background-color: #d1ecf1; padding: 1.5rem; border-radius: 6px; margin-top: 1.5rem;">
                    <p style="margin: 0; color: #0c5460;"><strong>üí° Tip:</strong> Start with the simplified examples! Try <code>video_keyboard_effect_switcher.py</code> (Example 02) for basic effect switching, or <code>simple_color_tracker.py</code> (Example 08) for color tracking. These beginner-friendly versions are perfect starting points for Lab 2!</p>
                </div>

                <h5>Example Starting Template</h5>
                <pre><code>import cv2
import numpy as np

class VideoProcessor:
    def __init__(self, source=0):
        self.cap = cv2.VideoCapture(source)
        if not self.cap.isOpened():
            raise ValueError("Could not open video source")
        
        self.current_effect = 0
        self.effects = ['Original', 'Grayscale', 'Blur', 'Edge Detection']
    
    def apply_effect(self, frame):
        if self.current_effect == 0:
            return frame.copy()
        elif self.current_effect == 1:
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            return cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR)
        # Add more effects...
        return frame
    
    def run(self):
        while True:
            ret, frame = self.cap.read()
            if not ret:
                break
            
            frame = cv2.flip(frame, 1)
            processed = self.apply_effect(frame)
            
            cv2.putText(processed, f"Mode: {self.effects[self.current_effect]}", 
                       (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
            
            cv2.imshow('Video Processor', processed)
            
            key = cv2.waitKey(1) & 0xFF
            if key == ord('q'):
                break
            elif ord('0') <= key <= ord('3'):
                self.current_effect = key - ord('0')
        
        self.cleanup()
    
    def cleanup(self):
        self.cap.release()
        cv2.destroyAllWindows()

if __name__ == "__main__":
    processor = VideoProcessor(0)  # Use camera
    processor.run()</code></pre>

                <div class="section-divider"></div>

                <h3 id="resources">Additional Resources</h3>

                <h4>Documentation</h4>
                <ul>
                    <li><a href="https://docs.opencv.org/master/d8/dfe/classcv_1_1VideoCapture.html" target="_blank">OpenCV VideoCapture</a> - Video input class</li>
                    <li><a href="https://docs.opencv.org/master/dd/d9e/classcv_1_1VideoWriter.html" target="_blank">OpenCV VideoWriter</a> - Video output class</li>
                    <li><a href="https://docs.opencv.org/master/dd/d00/tutorial_table_of_content_video_io.html" target="_blank">OpenCV Video I/O Tutorials</a></li>
                    <li><a href="https://docs.opencv.org/master/d1/dc5/tutorial_background_subtraction.html" target="_blank">Background Subtraction Tutorial</a></li>
                </ul>

                <h4>Free Video Assets</h4>
                <ul>
                    <li><a href="https://www.pexels.com/videos/" target="_blank">Pexels Videos</a> - Free stock videos</li>
                    <li><a href="https://pixabay.com/videos/" target="_blank">Pixabay Videos</a> - Free video clips</li>
                    <li><a href="https://www.videvo.net/" target="_blank">Videvo</a> - Free stock footage</li>
                </ul>

                <h4>Course Materials</h4>
                <ul>
                    <li><strong>Google Drive:</strong> <a href="https://drive.google.com/drive/u/1/folders/1QBC_GxFM-6KoTL7bqZjC76_r6MVasSG8" target="_blank">Digital Doubles Folder</a></li>
                    <li><strong>Code Examples:</strong> Week3/code_examples/ (10 comprehensive examples with multiple versions)</li>
                    <li><strong>GitHub:</strong> <a href="https://github.com/js6450/Fall25_Digital_Doubles" target="_blank">Fall25_Digital_Doubles</a></li>
                </ul>

                <h4>Getting Help</h4>
                <ul>
                    <li><strong>Office Hours:</strong> By appointment - email <a href="mailto:jiwon.shin@nyu.edu">jiwon.shin@nyu.edu</a></li>
                    <li><strong>Technical Issues:</strong> <a href="https://nyu.edu/it/servicedesk" target="_blank">NYU IT Service Desk</a></li>
                </ul>

                <p style="margin-top: 3rem; text-align: center; color: #999; font-size: 0.9rem;">
                    <em>Last updated: November 6, 2025 - Week 3 covers video processing fundamentals, live video capture, real-time effects, motion detection, color tracking, mirror effects, and performance monitoring.</em>
                </p>
            </div>
        </section>

        <div class="text-center mt-2">
            <a href="week2.html" class="btn">‚Üê Previous Week</a>
            <a href="weeks.html" class="btn">Back to All Weeks</a>
            <a href="week4.html" class="btn">Next Week ‚Üí</a>
        </div>
    </div>

    <footer>
        <p>&copy; 2025 Digital Doubles - DM-GY 9201 B</p>
        <p>NYU Tandon School of Engineering | Integrated Design & Media</p>
        <p>Instructor: Jiwon Shin</p>
    </footer>
</body>
</html>
